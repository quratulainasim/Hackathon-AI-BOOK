---
id: glossary
title: Glossary
sidebar_label: Glossary
---

# Glossary

This glossary provides definitions for key terms and concepts used throughout the book, related to Physical AI, humanoid robotics, ROS 2, digital twin simulation, NVIDIA Isaac, and Vision-Language-Action systems.

## Terms and Definitions

*   **Action Grounding**: The process of translating abstract action plans generated by an AI model (e.g., an LLM) into concrete, executable commands for a robot's control system, mapping natural language concepts to the robot's kinematics and motion primitives.
*   **Actuators**: Components of a robot (e.g., motors, hydraulics) that convert energy into physical motion, enabling the robot to move its joints and interact with its environment.
*   **APA 7th Edition**: A widely accepted style guide for academic writing, particularly in the social and behavioral sciences, which specifies formatting guidelines for citations, references, and manuscript structure.
*   **Digital Twin**: A virtual replica of a physical system, process, or object that is continuously updated with data from its real-world counterpart. In robotics, it allows for simulation, monitoring, and analysis in a virtual environment.
*   **Docusaurus**: An open-source static site generator for building documentation websites, primarily using Markdown for content and React for dynamic components.
*   **Embodied AI**: See **Physical AI**.
*   **Embodied Cognition**: The theory that an intelligent agent's cognitive processes are deeply dependent on its physical body and its interactions with the environment.
*   **Gazebo**: An open-source 3D robotics simulator that provides robust physics simulation, high-quality graphics, and a convenient programming interface for testing and developing robot software.
*   **GPT (Generative Pre-trained Transformer)**: A type of large language model (LLM) capable of understanding natural language input, generating human-like text, and performing various language-based tasks, including action planning in robotics.
*   **Human-Robot Interaction (HRI)**: The study and design of interactions between humans and robots, focusing on creating natural, intuitive, and safe communication and collaboration.
*   **Humanoid Robot**: A robot designed to resemble the human body, typically with a torso, head, two arms, and two legs, enabling it to operate in human-centric environments.
*   **Kinematics**: The study of motion without considering its causes (e.g., forces and torques). In robotics, it involves the mathematical description of a robot's motion based on its joint angles and link lengths.
*   **LLM (Large Language Model)**: A type of artificial intelligence model trained on vast amounts of text data, capable of understanding, generating, and processing human language for various tasks.
*   **Nav2 (Navigation2)**: A flexible and modular navigation framework for ROS 2 that provides advanced capabilities for autonomous robot navigation, including path planning, obstacle avoidance, and recovery behaviors.
*   **Nodes (ROS 2)**: Executable processes in ROS 2 that perform specific tasks, communicating with each other through topics and services.
*   **NVIDIA Isaac Platform**: A comprehensive platform for robotics development, offering tools and resources for simulation, AI-powered perception, navigation, and manipulation, leveraging NVIDIA GPUs and AI.
*   **Perception**: The ability of a robot to interpret sensory data (e.g., from cameras, lidar, force sensors) to understand its surroundings, identify objects, and estimate its own state.
*   **Physical AI**: Artificial intelligence systems that interact with the real world through a physical body, such as a robot, contending with real-world physics and dynamics. Also known as Embodied AI.
*   **Reinforcement Learning (RL)**: A type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize a cumulative reward, often used for training robot behaviors in simulation.
*   **ROS 2 (Robot Operating System 2)**: A flexible framework consisting of tools, libraries, and conventions that simplifies the task of creating complex and robust robot software across diverse robotic platforms.
*   **Sensors**: Devices that detect and respond to events or changes in the physical environment and convert them into electrical signals or data that can be processed by a robot's control system.
*   **Services (ROS 2)**: A synchronous request/reply communication mechanism in ROS 2, where a client node sends a request to a service and receives a response.
*   **Sim-to-Real Transfer**: The process of training AI models or robot behaviors in simulated environments and then successfully deploying them on physical robots, addressing the differences between simulation and reality.
*   **SLAM (Simultaneous Localization and Mapping)**: A computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.
*   **Speech-to-Text (STT)**: The process of converting spoken language into written text.
*   **Topics (ROS 2)**: The primary mechanism for asynchronous, many-to-many communication in ROS 2, where nodes publish messages to named topics and other nodes subscribe to receive the data.
*   **URDF (Unified Robot Description Format)**: An XML format used in ROS 2 to describe all aspects of a robot model, including its kinematic and dynamic properties, visual appearance, and collision geometry.
*   **Vercel**: A cloud platform for frontend developers, providing continuous deployment for static sites and serverless functions, often used for hosting Docusaurus projects.
*   **Vision-Language-Action (VLA) Robotics**: An emerging field that enables robots to understand and execute complex instructions given in natural language by leveraging advances in large language models (LLMs) and multimodal AI.
*   **VSLAM (Visual SLAM)**: A type of SLAM that uses visual sensor data (e.g., from cameras) to simultaneously build a map of an unknown environment and estimate the robot's position within that map.
*   **Whisper (OpenAI)**: A general-purpose speech recognition model developed by OpenAI that can transcribe audio into text with high accuracy and robustness across various languages and audio conditions, suitable for voice command processing in robotics.