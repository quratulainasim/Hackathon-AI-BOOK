"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[782],{8082:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>c});var r=i(4848),o=i(8453);const t={id:"humanoid-architecture",title:"Humanoid Architecture",sidebar_label:"Humanoid Architecture"},s=void 0,a={id:"introduction/humanoid-architecture",title:"Humanoid Architecture",description:"Humanoid Robot Architecture",source:"@site/docs/introduction/humanoid-architecture.md",sourceDirName:"introduction",slug:"/introduction/humanoid-architecture",permalink:"/docs/introduction/humanoid-architecture",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"humanoid-architecture",title:"Humanoid Architecture",sidebar_label:"Humanoid Architecture"},sidebar:"tutorialSidebar",previous:{title:"Physical AI",permalink:"/docs/introduction/physical-ai"},next:{title:"ROS 2 Fundamentals",permalink:"/docs/ros2-fundamentals/"}},l={},c=[{value:"Humanoid Robot Architecture",id:"humanoid-robot-architecture",level:2},{value:"1. Hardware Layer",id:"1-hardware-layer",level:3},{value:"2. Low-Level Control Layer",id:"2-low-level-control-layer",level:3},{value:"3. Perception Layer",id:"3-perception-layer",level:3},{value:"4. Cognition and High-Level Planning Layer",id:"4-cognition-and-high-level-planning-layer",level:3},{value:"5. Human-Robot Interaction (HRI) Layer",id:"5-human-robot-interaction-hri-layer",level:3}];function d(n){const e={h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.h2,{id:"humanoid-robot-architecture",children:"Humanoid Robot Architecture"}),"\n",(0,r.jsx)(e.p,{children:"Humanoid robot architecture is a complex interplay of hardware and software components designed to enable autonomous operation, perception, and interaction in human environments. A typical architecture can be broadly divided into several layers:"}),"\n",(0,r.jsx)(e.h3,{id:"1-hardware-layer",children:"1. Hardware Layer"}),"\n",(0,r.jsx)(e.p,{children:"This layer comprises the physical components of the robot:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Actuators"}),": Motors and joints that enable movement (e.g., in arms, legs, torso, neck)."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensors"}),": Devices for perceiving the environment (e.g., cameras for vision, lidar for ranging, microphones for audio, force-torque sensors for interaction)."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"End-Effectors"}),": Hands or grippers designed for manipulation tasks."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Power System"}),": Batteries and power distribution units."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Embedded Processors"}),": Microcontrollers for low-level motor control and sensor data acquisition."]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"2-low-level-control-layer",children:"2. Low-Level Control Layer"}),"\n",(0,r.jsx)(e.p,{children:"This layer handles the direct interface with the hardware:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Motor Control"}),": Precise control of joint positions, velocities, and torques."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor Data Processing"}),": Filtering, calibration, and initial interpretation of raw sensor data."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Kinematics/Dynamics"}),": Calculations related to robot movement, inverse kinematics for reaching goals, and dynamics for stable walking/balancing."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Real-time Operating System (RTOS)"}),": Often used to ensure deterministic execution of critical control loops."]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"3-perception-layer",children:"3. Perception Layer"}),"\n",(0,r.jsx)(e.p,{children:"Responsible for interpreting sensory information to build a coherent understanding of the environment:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Computer Vision"}),": Object detection, recognition, tracking, 3D reconstruction from camera data."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"SLAM (Simultaneous Localization and Mapping)"}),": Building a map of the environment while simultaneously tracking the robot's position within it (e.g., VSLAM)."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Speech Recognition"}),": Converting audio input into text (e.g., Whisper integration)."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensors to create a more robust and accurate environmental model."]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"4-cognition-and-high-level-planning-layer",children:"4. Cognition and High-Level Planning Layer"}),"\n",(0,r.jsx)(e.p,{children:"This layer focuses on reasoning, decision-making, and abstract task planning:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Knowledge Representation"}),": Storing and managing information about objects, environments, and tasks."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Task Planning"}),": Decomposing high-level goals into sequences of primitive actions."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Motion Planning"}),": Generating collision-free paths for the robot's body and end-effectors."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Natural Language Understanding (NLU)"}),": Interpreting human commands and intentions (e.g., GPT integration)."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Reinforcement Learning"}),": Learning complex behaviors through trial and error in simulated or real environments."]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"5-human-robot-interaction-hri-layer",children:"5. Human-Robot Interaction (HRI) Layer"}),"\n",(0,r.jsx)(e.p,{children:"Facilitates natural communication and collaboration between humans and robots:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Speech Synthesis"}),": Converting text responses into spoken language."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Gesture Recognition/Generation"}),": Understanding and producing non-verbal cues."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"User Interfaces"}),": Visual displays or other means for human operators to monitor and control the robot."]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"This layered architecture allows for modular development, making it easier to integrate new technologies, troubleshoot issues, and scale capabilities."})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>a});var r=i(6540);const o={},t=r.createContext(o);function s(n){const e=r.useContext(t);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),r.createElement(t.Provider,{value:e},n.children)}}}]);